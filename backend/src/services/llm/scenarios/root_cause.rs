//! Root Cause Analysis Scenario
//!
//! LLM-enhanced root cause analysis for query profile diagnostics.

use serde::{Deserialize, Serialize};
use std::collections::HashMap;

use crate::services::llm::models::LLMScenario;
use crate::services::llm::service::{LLMAnalysisRequestTrait, LLMAnalysisResponseTrait};

// ============================================================================
// System Prompt - Dynamic Generation
// ============================================================================

/// Base system prompt - the static foundation
const PROMPT_BASE: &str = r#"ä½ æ˜¯ä¸€ä½æ‹¥æœ‰20å¹´ä»¥ä¸Šçš„StarRocks OLAP æ•°æ®åº“çš„é«˜çº§æ€§èƒ½ä¸“å®¶ã€‚
ä½ éœ€è¦åˆ†æ Query Profile æ•°æ®ï¼Œè¯†åˆ«çœŸæ­£çš„æ ¹å› å¹¶ç»™å‡º**å¯ç›´æ¥æ‰§è¡Œ**çš„ä¼˜åŒ–å»ºè®®ã€‚

## ğŸ§  æ‰¹åˆ¤æ€§æ€ç»´è¦æ±‚ (Critical Thinking)

åœ¨ç»™å‡ºä»»ä½•è¯Šæ–­æˆ–å»ºè®®å‰ï¼Œä½ å¿…é¡»è¿›è¡Œ**è‡ªæˆ‘æ‰¹è¯„å¼æ€è€ƒ**ï¼š

1. **è´¨ç–‘å‡è®¾**: æˆ‘çš„è¯Šæ–­æ˜¯å¦åŸºäºå……åˆ†çš„è¯æ®ï¼Ÿæ˜¯å¦æœ‰å…¶ä»–å¯èƒ½çš„è§£é‡Šï¼Ÿ
2. **éªŒè¯å‚æ•°**: æˆ‘æ¨èçš„å‚æ•°æ˜¯å¦çœŸå®å­˜åœ¨äº StarRocks å®˜æ–¹æ–‡æ¡£ä¸­ï¼Ÿå¦‚æœä¸ç¡®å®šï¼Œå®å¯ä¸æ¨èã€‚
3. **æ£€æŸ¥é€‚ç”¨æ€§**: è¿™ä¸ªå»ºè®®æ˜¯å¦é€‚ç”¨äºå½“å‰çš„è¡¨ç±»å‹ï¼ˆå†…è¡¨/å¤–è¡¨ï¼‰ï¼Ÿ
4. **é¿å…è‡†æ–­**: æˆ‘æ˜¯å¦åœ¨æ²¡æœ‰æ•°æ®æ”¯æ’‘çš„æƒ…å†µä¸‹åšå‡ºäº†æ¨æµ‹ï¼Ÿ
5. **åæ€åè§**: æˆ‘æ˜¯å¦è¿‡åº¦ä¾èµ–æŸäº›å¸¸è§æ¨¡å¼è€Œå¿½ç•¥äº†å…·ä½“æƒ…å†µï¼Ÿ

**é‡è¦åŸåˆ™**: å®å¯å°‘ç»™å»ºè®®ï¼Œä¹Ÿä¸è¦ç»™å‡ºé”™è¯¯æˆ–ä¸å­˜åœ¨çš„å‚æ•°å»ºè®®ï¼

## åˆ†ææ–¹æ³•è®º (Chain-of-Thought)

### Step 1: ç†è§£æŸ¥è¯¢æ„å›¾
- è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„æŸ¥è¯¢ï¼Ÿ(OLAPèšåˆ/ç‚¹æŸ¥/ETLå¯¼å…¥/Joinå¯†é›†å‹)
- æ¶‰åŠå“ªäº›è¡¨ï¼Ÿå„è¡¨çš„æ•°æ®é‡çº§ï¼Ÿ
- è‡ªé—®: æˆ‘æ˜¯å¦å®Œæ•´ç†è§£äº†æŸ¥è¯¢çš„ä¸šåŠ¡åœºæ™¯ï¼Ÿ

### Step 2: è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
- å“ªä¸ªç®—å­è€—æ—¶æœ€é•¿ï¼Ÿ(time_pct > 30%)
- æ˜¯ IO ç“¶é¢ˆè¿˜æ˜¯ CPU ç“¶é¢ˆï¼Ÿ
- æ˜¯å¦æœ‰æ•°æ®å€¾æ–œï¼Ÿ(max/avg æ¯”å€¼)
- è‡ªé—®: æˆ‘çš„åˆ¤æ–­æ˜¯å¦æœ‰ Profile æŒ‡æ ‡æ”¯æ’‘ï¼Ÿ

### Step 3: æ ¹å› æº¯æº
- ç“¶é¢ˆç®—å­çš„ä¸Šæ¸¸æ˜¯ä»€ä¹ˆï¼Ÿ
- æ ¹å› æ˜¯æ•°æ®é—®é¢˜è¿˜æ˜¯é…ç½®é—®é¢˜ï¼Ÿ
- æ˜¯å¦æœ‰è§„åˆ™å¼•æ“æœªå‘ç°çš„éšå¼æ ¹å› ï¼Ÿ
- è‡ªé—®: æˆ‘æ˜¯å¦æ··æ·†äº†ç—‡çŠ¶å’Œæ ¹å› ï¼Ÿ

### Step 4: åˆ¶å®šä¼˜åŒ–æ–¹æ¡ˆ
- é’ˆå¯¹æ ¹å› è€Œéç—‡çŠ¶ç»™å‡ºå»ºè®®
- ä¼˜å…ˆç»™å‡ºæŠ•å…¥äº§å‡ºæ¯”æœ€é«˜çš„ä¼˜åŒ–
- å¿…é¡»æ˜¯å¯ç›´æ¥æ‰§è¡Œçš„å‘½ä»¤
- è‡ªé—®: è¿™ä¸ªå»ºè®®åœ¨ç”¨æˆ·ç¯å¢ƒä¸­æ˜¯å¦å¯è¡Œï¼Ÿ

### Step 5: è‡ªæˆ‘éªŒè¯ (å¿…åš)
- **å‚æ•°å­˜åœ¨æ€§**: æˆ‘æ¨èçš„æ¯ä¸ªå‚æ•°æ˜¯å¦åœ¨ä¸‹æ–¹çš„"å®˜æ–¹æ”¯æŒå‚æ•°åˆ—è¡¨"ä¸­ï¼Ÿ
- **è¡¨ç±»å‹åŒ¹é…**: å¯¹å¤–è¡¨å»ºè®® ALTER TABLE åˆ†æ¡¶æ˜¯é”™è¯¯çš„ï¼
- **é…ç½®å†²çª**: æ˜¯å¦ä¸å½“å‰ session_variables ä¸­çš„å€¼é‡å¤ï¼Ÿ
- **å‘½ä»¤å®Œæ•´æ€§**: SQL/SET å‘½ä»¤æ˜¯å¦å¯ä»¥ç›´æ¥å¤åˆ¶æ‰§è¡Œï¼Ÿ

## âš ï¸ ä¸¥æ ¼éµå®ˆçš„è§„åˆ™
1. **æ£€æŸ¥ session_variables å†ç»™å»ºè®®**: å‚æ•°å·²å¯ç”¨å°±ä¸è¦é‡å¤å»ºè®®
2. **åŒºåˆ†è¡¨ç±»å‹**: å†…è¡¨å’Œå¤–è¡¨çš„ä¼˜åŒ–æ–¹å‘å®Œå…¨ä¸åŒ
3. **å‚æ•°å¿…é¡»å­˜åœ¨**: åªä½¿ç”¨ä¸‹æ–¹åˆ—å‡ºçš„ StarRocks å®˜æ–¹å‚æ•°ï¼Œç¦æ­¢åˆ›é€ å‚æ•°ï¼
4. **å»ºè®®å¿…é¡»å¯æ‰§è¡Œ**: ç»™å‡ºå®Œæ•´çš„ SQL/SET/ALTER å‘½ä»¤
5. **å®ç¼ºæ¯‹æ»¥**: ä¸ç¡®å®šçš„å»ºè®®å®å¯ä¸ç»™ï¼Œä¹Ÿä¸è¦è¯¯å¯¼ç”¨æˆ·"#;

/// Dynamic prompt section for table types detected
fn build_table_type_prompt(scan_details: &[ScanDetailForLLM]) -> String {
    let mut internal_tables = Vec::new();
    let mut external_tables: HashMap<String, Vec<String>> = HashMap::new();

    for scan in scan_details {
        let table_name = &scan.table_name;
        if scan.table_type == "internal" {
            internal_tables.push(table_name.clone());
        } else {
            let connector = scan
                .connector_type
                .clone()
                .unwrap_or_else(|| "unknown".to_string());
            external_tables
                .entry(connector)
                .or_default()
                .push(table_name.clone());
        }
    }

    let mut prompt = String::from("\n\n## ğŸ“Š æœ¬æ¬¡æŸ¥è¯¢æ¶‰åŠçš„è¡¨\n");

    if !internal_tables.is_empty() {
        prompt.push_str(&format!(
            "\n### StarRocks å†…è¡¨ ({} å¼ )\nè¡¨å: {}\n\n**å†…è¡¨ä¼˜åŒ–æ–¹å‘:**\n- ANALYZE TABLE æ›´æ–°ç»Ÿè®¡ä¿¡æ¯\n- æ£€æŸ¥åˆ†æ¡¶é”®æ˜¯å¦åˆç†\n- è€ƒè™‘ç‰©åŒ–è§†å›¾åŠ é€Ÿ\n- å¯ä½¿ç”¨ ALTER TABLE è°ƒæ•´å±æ€§\n",
            internal_tables.len(),
            internal_tables.join(", ")
        ));
    }

    for (connector, tables) in &external_tables {
        let connector_prompt = match connector.as_str() {
            "hive" => format!(
                "\n### Hive å¤–è¡¨ ({} å¼ )\nè¡¨å: {}\n\n**Hive è¡¨ä¼˜åŒ–æ–¹å‘:**\n- å¯ç”¨ DataCache: `SET enable_scan_datacache=true;`\n- åˆ†åŒºè£å‰ª: ç¡®ä¿ WHERE æ¡ä»¶åŒ…å«åˆ†åŒºåˆ—\n- å°æ–‡ä»¶åˆå¹¶: åœ¨ Hive/Spark ç«¯æ‰§è¡Œ `ALTER TABLE xxx CONCATENATE;`\n- âš ï¸ ä¸èƒ½ç”¨ ALTER TABLE æ”¹åˆ†æ¡¶ï¼Œéœ€åœ¨ Hive ç«¯æ“ä½œ\n",
                tables.len(),
                tables.join(", ")
            ),
            "iceberg" => format!(
                "\n### Iceberg å¤–è¡¨ ({} å¼ )\nè¡¨å: {}\n\n**Iceberg è¡¨ä¼˜åŒ–æ–¹å‘:**\n- å¯ç”¨ DataCache: `SET enable_scan_datacache=true;`\n- æ–‡ä»¶åˆå¹¶: ä½¿ç”¨ Spark `rewrite_data_files` procedure\n- åˆ©ç”¨ Iceberg çš„ hidden partitioning\n- æ£€æŸ¥ delete files æ˜¯å¦è¿‡å¤š (V2 æ ¼å¼)\n- âš ï¸ ä¸èƒ½ç”¨ ALTER TABLE æ”¹åˆ†æ¡¶ï¼Œéœ€åœ¨ Iceberg ç«¯æ“ä½œ\n",
                tables.len(),
                tables.join(", ")
            ),
            "hudi" => format!(
                "\n### Hudi å¤–è¡¨ ({} å¼ )\nè¡¨å: {}\n\n**Hudi è¡¨ä¼˜åŒ–æ–¹å‘:**\n- å¯ç”¨ DataCache\n- æ£€æŸ¥ compaction æ˜¯å¦åŠæ—¶\n- MOR è¡¨è€ƒè™‘è°ƒæ•´è¯»å–æ¨¡å¼\n",
                tables.len(),
                tables.join(", ")
            ),
            "jdbc" => format!(
                "\n### JDBC å¤–è¡¨ ({} å¼ )\nè¡¨å: {}\n\n**JDBC è¡¨ä¼˜åŒ–æ–¹å‘:**\n- è°“è¯ä¸‹æ¨: ç¡®ä¿ WHERE æ¡ä»¶èƒ½ä¸‹æ¨åˆ°æºåº“\n- å‡å°‘ SELECT åˆ—: åªæŸ¥è¯¢å¿…è¦çš„åˆ—\n- è€ƒè™‘æ•°æ®åŒæ­¥åˆ°å†…è¡¨åŠ é€Ÿ\n",
                tables.len(),
                tables.join(", ")
            ),
            "es" => format!(
                "\n### Elasticsearch å¤–è¡¨ ({} å¼ )\nè¡¨å: {}\n\n**ES è¡¨ä¼˜åŒ–æ–¹å‘:**\n- ç¡®ä¿æŸ¥è¯¢æ¡ä»¶èƒ½ä¸‹æ¨åˆ° ES\n- åˆ©ç”¨ ES çš„ç´¢å¼•èƒ½åŠ›\n- å‡å°‘è¿”å›å­—æ®µæ•°\n",
                tables.len(),
                tables.join(", ")
            ),
            _ => format!(
                "\n### {} å¤–è¡¨ ({} å¼ )\nè¡¨å: {}\n\n**é€šç”¨å¤–è¡¨ä¼˜åŒ–æ–¹å‘:**\n- å¯ç”¨ DataCache\n- åˆ†åŒºè£å‰ª\n- è°“è¯ä¸‹æ¨\n",
                connector,
                tables.len(),
                tables.join(", ")
            ),
        };
        prompt.push_str(&connector_prompt);
    }

    prompt
}

/// Dynamic prompt section based on detected issues
fn build_issue_focused_prompt(diagnostics: &[DiagnosticForLLM]) -> String {
    if diagnostics.is_empty() {
        return String::from(
            "\n\n## è§„åˆ™å¼•æ“æœªå‘ç°æ˜æ˜¾é—®é¢˜\nè¯·æ·±å…¥åˆ†æåŸå§‹ Profile æ•°æ®ï¼Œå¯»æ‰¾éšå¼æ€§èƒ½é—®é¢˜ã€‚\n",
        );
    }

    let mut prompt = String::from("\n\n## è§„åˆ™å¼•æ“å·²è¯†åˆ«çš„é—®é¢˜ (ä»…ä½œä¸ºå‚è€ƒ)\n");
    for d in diagnostics.iter().take(5) {
        prompt.push_str(&format!("- **{}** [{}]: {}\n", d.rule_id, d.severity, d.message));
    }
    prompt.push_str("\n**ä½ çš„ä»»åŠ¡**: ä¸è¦ç®€å•é‡å¤è¿™äº›é—®é¢˜ï¼Œè€Œæ˜¯:\n1. åˆ†æè¿™äº›ç—‡çŠ¶èƒŒåçš„æ ¹å› \n2. æ‰¾å‡ºè§„åˆ™å¼•æ“æœªå‘ç°çš„éšå¼é—®é¢˜\n3. å»ºç«‹å› æœé“¾æ¡\n");

    prompt
}

/// Dynamic prompt section for current session variables
///
/// Uses ALL passed session_vars (already filtered by CLUSTER_VARIABLE_NAMES at fetch time).
/// Dynamically detects `enable_*` prefix for boolean feature flags.
fn build_session_vars_prompt(session_vars: &HashMap<String, String>) -> String {
    if session_vars.is_empty() {
        return String::new();
    }

    let mut prompt = String::from("\n\n## âš ï¸ å½“å‰é›†ç¾¤é…ç½® (ä¸¥æ ¼ç¦æ­¢é‡å¤å»ºè®®!)\n");

    let mut enabled_features = Vec::new();
    let mut disabled_features = Vec::new();
    let mut other_settings = Vec::new();

    for (var, value) in session_vars {
        let is_bool_flag = var.starts_with("enable_");
        let is_true = value == "true" || value == "1";

        if is_bool_flag {
            if is_true {
                enabled_features.push(var.as_str());
            } else {
                disabled_features.push(var.as_str());
            }
        } else {
            other_settings.push((var.as_str(), value.as_str()));
        }
    }

    enabled_features.sort();
    disabled_features.sort();
    other_settings.sort_by_key(|(k, _)| *k);

    if !enabled_features.is_empty() {
        prompt.push_str(&format!(
            "\n### ğŸŸ¢ å·²å¯ç”¨çš„åŠŸèƒ½ (ç¦æ­¢å†å»ºè®®å¼€å¯!)\n{}\n",
            enabled_features
                .iter()
                .map(|v| format!("`{}`", v))
                .collect::<Vec<_>>()
                .join(", ")
        ));
    }

    if !disabled_features.is_empty() {
        prompt.push_str(&format!(
            "\n### ğŸ”´ å·²ç¦ç”¨çš„åŠŸèƒ½ (å¯å»ºè®®å¼€å¯)\n{}\n",
            disabled_features
                .iter()
                .map(|v| format!("`{}`", v))
                .collect::<Vec<_>>()
                .join(", ")
        ));
    }

    if !other_settings.is_empty() {
        prompt.push_str("\n### å…¶ä»–é…ç½®\n");
        for (var, value) in &other_settings {
            prompt.push_str(&format!("- `{}` = `{}`\n", var, value));
        }
    }

    prompt.push_str(
        r#"
### ğŸš« ä¸¥æ ¼è§„åˆ™
1. **ç¦æ­¢å»ºè®®** `SET enable_xxx = true` å¦‚æœè¯¥å‚æ•°åœ¨"å·²å¯ç”¨çš„åŠŸèƒ½"åˆ—è¡¨ä¸­
2. åªèƒ½å»ºè®®å¼€å¯"å·²ç¦ç”¨çš„åŠŸèƒ½"åˆ—è¡¨ä¸­çš„å‚æ•°
3. è¿åä»¥ä¸Šè§„åˆ™å°†è¢«è§†ä¸ºä¸¥é‡é”™è¯¯!
"#,
    );

    prompt
}

/// Static prompt section for valid parameters (verified from StarRocks official docs)
const PROMPT_VALID_PARAMS: &str = r#"

## âœ… StarRocks å®˜æ–¹æ”¯æŒçš„å‚æ•° (å·²éªŒè¯)

ä»¥ä¸‹å‚æ•°å‡æ¥è‡ª StarRocks å®˜æ–¹æ–‡æ¡£ï¼Œå¯å®‰å…¨ä½¿ç”¨ã€‚å¦‚æœä½ æƒ³æ¨èçš„å‚æ•°ä¸åœ¨æ­¤åˆ—è¡¨ä¸­ï¼Œè¯·ä¸è¦æ¨èï¼

### Session å˜é‡ (SET xxx = yyy)

**æŸ¥è¯¢èµ„æºæ§åˆ¶:**
- `query_mem_limit` - å•ä¸ªæŸ¥è¯¢å†…å­˜é™åˆ¶ (bytes)
- `query_timeout` - æŸ¥è¯¢è¶…æ—¶æ—¶é—´ (ç§’ï¼Œé»˜è®¤300)
- `exec_mem_limit` - å•ä¸ª BE èŠ‚ç‚¹å†…å­˜é™åˆ¶

**å¹¶è¡Œåº¦æ§åˆ¶:**
- `pipeline_dop` - Pipeline å¹¶è¡Œåº¦ (0=è‡ªåŠ¨)
- `parallel_fragment_exec_instance_num` - Fragment å®ä¾‹æ•° (é»˜è®¤1)
- `max_parallel_scan_instance_num` - Scan å¹¶è¡Œå®ä¾‹æ•°

**Spill (è½ç›˜):**
- `enable_spill` - å¯ç”¨è½ç›˜ (true/false)
- `spill_mem_table_size` - è½ç›˜è§¦å‘é˜ˆå€¼
- `spill_mem_table_num` - è½ç›˜è¡¨æ•°é‡

**DataCache (ä»…å¤–è¡¨! Hive/Iceberg/Hudi ç­‰):**
- `enable_scan_datacache` - å¯ç”¨ DataCache è¯»å– (å¤–è¡¨ä¸“ç”¨)
- `enable_populate_datacache` - å¯ç”¨ DataCache å†™å…¥ (å¤–è¡¨ä¸“ç”¨)
- âš ï¸ å†…è¡¨æ— éœ€é…ç½® DataCacheï¼Œå†…è¡¨ä½¿ç”¨ PageCacheï¼ˆè‡ªåŠ¨ï¼‰

**Query Cache (ä»…å†…è¡¨! ä¸æ”¯æŒå¤–è¡¨!):**
- `enable_query_cache` - å¯ç”¨ Query Cache (ä»…å†…è¡¨èšåˆæŸ¥è¯¢)
- `query_cache_entry_max_bytes` - å•ä¸ªç¼“å­˜æ¡ç›®æœ€å¤§å­—èŠ‚
- `query_cache_entry_max_rows` - å•ä¸ªç¼“å­˜æ¡ç›®æœ€å¤§è¡Œæ•°
- âš ï¸ Query Cache é™åˆ¶æ¡ä»¶:
  - ä»…æ”¯æŒåŸç”Ÿ OLAP è¡¨å’Œå­˜ç®—åˆ†ç¦»è¡¨ï¼Œ**ä¸æ”¯æŒå¤–è¡¨**!
  - ä»…æ”¯æŒèšåˆæŸ¥è¯¢ï¼ˆé GROUP BY æˆ–ä½åŸºæ•° GROUP BYï¼‰
  - ä¸æ”¯æŒ rand/random/uuid/sleep ç­‰ä¸ç¡®å®šæ€§å‡½æ•°
  - Tablet æ•°é‡ >= pipeline_dop æ—¶æ‰ç”Ÿæ•ˆ
  - é«˜åŸºæ•° GROUP BY ä¼šè‡ªåŠ¨ç»•è¿‡ç¼“å­˜

**Runtime Filter:**
- `enable_global_runtime_filter` - å…¨å±€ Runtime Filter
- `runtime_filter_wait_time_ms` - ç­‰å¾…æ—¶é—´
- `runtime_join_filter_push_down_limit` - ä¸‹æ¨è¡Œæ•°é™åˆ¶

**Join ä¼˜åŒ–:**
- `broadcast_row_limit` - Broadcast è¡Œæ•°é™åˆ¶ (é»˜è®¤25M)
- `hash_join_push_down_right_table` - å³è¡¨ä¸‹æ¨

**èšåˆä¼˜åŒ–:**
- `new_planner_agg_stage` - èšåˆé˜¶æ®µ (0=è‡ªåŠ¨,1/2/3/4)
- `streaming_preaggregation_mode` - é¢„èšåˆæ¨¡å¼

### ALTER TABLE å±æ€§ (ä»…é€‚ç”¨äº StarRocks å†…è¡¨!)

- `replication_num` - å‰¯æœ¬æ•°
- `bloom_filter_columns` - Bloom Filter åˆ—
- `colocate_with` - Colocate Group åç§°
- `dynamic_partition.enable` - åŠ¨æ€åˆ†åŒºå¼€å…³
- `storage_medium` - å­˜å‚¨ä»‹è´¨ (SSD/HDD)

### è¿ç»´å‘½ä»¤

- `ANALYZE TABLE db.table;` - æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ (ä»…å†…è¡¨)
- `REFRESH MATERIALIZED VIEW mv_name;` - åˆ·æ–°ç‰©åŒ–è§†å›¾
- `ADMIN SET REPLICA STATUS ...` - ç®¡ç†å‰¯æœ¬

### SQL Hint æ ¼å¼

```sql
SELECT /*+ SET_VAR(query_timeout=600, enable_spill=true) */ ...
```

## âŒ ç¦æ­¢ä½¿ç”¨çš„å‚æ•° (ä¸å­˜åœ¨æˆ–å·²åºŸå¼ƒ)

ä»¥ä¸‹å‚æ•°**ä¸å­˜åœ¨**äº StarRocks ä¸­ï¼Œç¦æ­¢æ¨èï¼š
- âŒ `enable_short_key_index` - ä¸å­˜åœ¨ï¼Short Key æ˜¯è‡ªåŠ¨çš„
- âŒ `enable_zone_map_index` - ä¸å­˜åœ¨ï¼Zone Map æ˜¯è‡ªåŠ¨çš„
- âŒ `enable_bitmap_index` - ä¸å­˜åœ¨ï¼ç”¨ CREATE INDEX å»ºç´¢å¼•
- âŒ `enable_async_profile` - ä¸å­˜åœ¨
- âŒ `enable_query_debug_trace` - ä¸å­˜åœ¨
- âŒ `optimize_table` - ä¸å­˜åœ¨ï¼å†…è¡¨ç”¨ ADMIN COMPACT
- âŒ ä»»ä½•ä½ "çŒœæµ‹"å¯èƒ½å­˜åœ¨çš„å‚æ•°

## âš ï¸ å¤–è¡¨é™åˆ¶ (Hive/Iceberg/JDBC ç­‰)

å¤–è¡¨**ä¸æ”¯æŒ**ä»¥ä¸‹æ“ä½œï¼Œç¦æ­¢å»ºè®®ï¼š
- âŒ `ALTER TABLE external_table SET ("xxx" = "yyy")` - å¤–è¡¨å±æ€§åœ¨æºç«¯ä¿®æ”¹
- âŒ `ANALYZE TABLE external_catalog.db.table` - å¤–è¡¨ç»Ÿè®¡ä¿¡æ¯åœ¨æºç«¯
- âŒ ä»»ä½•ä¿®æ”¹å¤–è¡¨åˆ†æ¡¶/åˆ†åŒºçš„å»ºè®®
- âŒ `enable_query_cache = true` - Query Cache ä¸æ”¯æŒå¤–è¡¨! å¤–è¡¨ç”¨ DataCache!

## ğŸ”„ ç¼“å­˜ç­–ç•¥æ€»ç»“

| ç¼“å­˜ç±»å‹ | é€‚ç”¨è¡¨ç±»å‹ | å‚æ•° | è¯´æ˜ |
|---------|-----------|------|------|
| Query Cache | å†…è¡¨ | `enable_query_cache` | ç¼“å­˜èšåˆè®¡ç®—ç»“æœ |
| DataCache | å¤–è¡¨ | `enable_scan_datacache` | ç¼“å­˜è¿œç¨‹æ•°æ®åˆ°æœ¬åœ° |
| PageCache | å†…è¡¨ | è‡ªåŠ¨ | ç¼“å­˜ç£ç›˜æ•°æ®é¡µï¼Œæ— éœ€é…ç½® |
"#;

/// Output format specification
const PROMPT_OUTPUT_FORMAT: &str = r#"

## ğŸ“¤ ä¸¥æ ¼ JSON è¾“å‡ºæ ¼å¼"#;

/// Build the complete dynamic system prompt
pub fn build_system_prompt(request: &RootCauseAnalysisRequest) -> String {
    let mut prompt = String::from(PROMPT_BASE);

    if let Some(ref profile_data) = request.profile_data {
        prompt.push_str(&build_table_type_prompt(&profile_data.scan_details));
    }

    prompt.push_str(&build_issue_focused_prompt(&request.rule_diagnostics));

    prompt.push_str(&build_session_vars_prompt(&request.query_summary.session_variables));

    prompt.push_str(PROMPT_VALID_PARAMS);

    prompt.push_str(PROMPT_OUTPUT_FORMAT);

    prompt.push_str(PROMPT_JSON_FORMAT);

    prompt
}

/// Output format JSON schema (appended to dynamic prompt)
const PROMPT_JSON_FORMAT: &str = r#"

```json
{
  "root_causes": [
    {
      "root_cause_id": "RC001",
      "description": "root cause description based on raw metrics analysis",
      "confidence": 0.85,
      "evidence": ["Profile metric evidence 1", "evidence 2"],
      "symptoms": ["S001", "G003"],
      "is_implicit": false
    }
  ],
  "causal_chains": [
    {
      "chain": ["Root Cause", "->", "Intermediate", "->", "Symptom"],
      "explanation": "Causal analysis based on Profile data"
    }
  ],
  "recommendations": [
    {
      "priority": 1,
      "action": "Brief description of recommended action",
      "expected_improvement": "Quantitative improvement description",
      "sql_example": "Executable SQL or command"
    }
  ],
  "summary": "Overall analysis summary focusing on root causes and optimization direction",
  "hidden_issues": [
    {
      "issue": "Issue not detected by rule engine",
      "suggestion": "Executable solution command"
    }
  ]
}
```

Field descriptions:
- root_cause_id: Format as "RC001", "RC002", etc.
- evidence: MUST reference specific Profile metric values
- symptoms: Related rule IDs
- is_implicit: true if not detected by rule engine
- priority: 1 is highest priority
- sql_example: REQUIRED, executable SQL/command
"#;

/// Legacy static prompt for backward compatibility (minimal)
#[allow(dead_code)]
pub const ROOT_CAUSE_SYSTEM_PROMPT: &str = "You are a StarRocks OLAP database performance expert.";

// ============================================================================
// Request Types
// ============================================================================

/// Root Cause Analysis Request to LLM
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RootCauseAnalysisRequest {
    /// Query summary information
    pub query_summary: QuerySummaryForLLM,
    /// Raw profile data for deep analysis (NEW - åŸå§‹ Profile æ•°æ®)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub profile_data: Option<ProfileDataForLLM>,
    /// Execution plan (simplified for token efficiency)
    pub execution_plan: ExecutionPlanForLLM,
    /// Rule engine diagnostics (for reference, LLM should go deeper)
    pub rule_diagnostics: Vec<DiagnosticForLLM>,
    /// Key performance metrics
    pub key_metrics: KeyMetricsForLLM,
    /// Optional user question for follow-up
    #[serde(skip_serializing_if = "Option::is_none")]
    pub user_question: Option<String>,
}

impl LLMAnalysisRequestTrait for RootCauseAnalysisRequest {
    fn scenario(&self) -> LLMScenario {
        LLMScenario::RootCauseAnalysis
    }

    /// Generate dynamic system prompt based on request context
    fn system_prompt(&self) -> String {
        build_system_prompt(self)
    }

    fn cache_key(&self) -> String {
        use std::hash::{Hash, Hasher};
        let mut hasher = std::collections::hash_map::DefaultHasher::new();
        self.sql_hash().hash(&mut hasher);
        self.profile_hash().hash(&mut hasher);
        format!("rca:{:x}", hasher.finish())
    }

    fn sql_hash(&self) -> String {
        use std::hash::{Hash, Hasher};
        let mut hasher = std::collections::hash_map::DefaultHasher::new();
        self.query_summary.sql_statement.hash(&mut hasher);
        format!("{:x}", hasher.finish())
    }

    fn profile_hash(&self) -> String {
        use std::hash::{Hash, Hasher};
        let mut hasher = std::collections::hash_map::DefaultHasher::new();

        self.query_summary.scan_bytes.hash(&mut hasher);
        self.query_summary.output_rows.hash(&mut hasher);
        self.rule_diagnostics.len().hash(&mut hasher);

        self.query_summary.query_type.hash(&mut hasher);
        format!("{:x}", hasher.finish())
    }
}

/// Query summary for LLM analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QuerySummaryForLLM {
    /// Full SQL statement (NOT truncated - LLM needs complete SQL for analysis)
    pub sql_statement: String,
    /// Query type: SELECT/INSERT/EXPORT/ANALYZE
    pub query_type: String,
    /// Query complexity level: "Simple" | "Medium" | "Complex" | "VeryComplex"
    /// Used for adaptive threshold selection and LLM context
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub query_complexity: Option<String>,
    /// Total execution time in seconds
    pub total_time_seconds: f64,
    /// Total bytes scanned
    pub scan_bytes: u64,
    /// Output row count
    pub output_rows: u64,
    /// Number of BE nodes
    pub be_count: u32,
    /// Whether spill occurred
    pub has_spill: bool,
    /// Spill details if spill occurred
    #[serde(skip_serializing_if = "Option::is_none")]
    pub spill_bytes: Option<String>,
    /// Non-default session variables (important for analysis)
    #[serde(default, skip_serializing_if = "HashMap::is_empty")]
    pub session_variables: HashMap<String, String>,
}

// ============================================================================
// Raw Profile Data - NEW: åŸå§‹ Profile æ•°æ®
// ============================================================================

/// Raw profile data for LLM deep analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProfileDataForLLM {
    /// All operator nodes with their metrics
    pub operators: Vec<OperatorDetailForLLM>,
    /// Cross-node time distribution (for detecting skew)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub time_distribution: Option<TimeDistributionForLLM>,
    /// Scan node details (tables, partitions, files)
    #[serde(default)]
    pub scan_details: Vec<ScanDetailForLLM>,
    /// Join node details (join type, build/probe stats)
    #[serde(default)]
    pub join_details: Vec<JoinDetailForLLM>,
    /// Aggregation node details
    #[serde(default)]
    pub agg_details: Vec<AggDetailForLLM>,
    /// Exchange (shuffle) details
    #[serde(default)]
    pub exchange_details: Vec<ExchangeDetailForLLM>,
}

/// Detailed operator information with all metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OperatorDetailForLLM {
    /// Operator name (SCAN, JOIN, AGG, etc.)
    pub operator: String,
    /// Plan node ID
    pub plan_node_id: i32,
    /// Execution time percentage
    pub time_pct: f64,
    /// Actual rows processed
    pub rows: u64,
    /// Estimated rows (for cardinality error detection)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub estimated_rows: Option<u64>,
    /// Memory used in bytes
    #[serde(skip_serializing_if = "Option::is_none")]
    pub memory_bytes: Option<u64>,
    /// All key metrics (raw from profile)
    pub metrics: HashMap<String, String>,
}

/// Time distribution across instances for skew detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimeDistributionForLLM {
    /// Max time across instances
    pub max_time_ms: f64,
    /// Min time across instances
    pub min_time_ms: f64,
    /// Average time
    pub avg_time_ms: f64,
    /// Skew ratio (max/avg)
    pub skew_ratio: f64,
    /// Per-instance times for top operators
    #[serde(default)]
    pub per_instance: Vec<InstanceTimeForLLM>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InstanceTimeForLLM {
    pub operator: String,
    pub instance_id: i32,
    pub time_ms: f64,
    pub rows: u64,
}

/// Scan operator details
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScanDetailForLLM {
    pub plan_node_id: i32,
    pub table_name: String,
    /// OlapScan / HdfsScan / ConnectorScan etc.
    pub scan_type: String,
    /// Table storage type: "internal" (StarRocks native), "external" (foreign table)
    /// This is CRITICAL for LLM to give correct suggestions!
    pub table_type: String,
    /// Connector type for external tables: "hive", "iceberg", "hudi", "deltalake", "paimon", "jdbc", "es", "unknown"
    /// For internal tables this is "native"
    #[serde(skip_serializing_if = "Option::is_none")]
    pub connector_type: Option<String>,
    /// Total rows read
    pub rows_read: u64,
    /// Rows after filtering
    pub rows_returned: u64,
    /// Filter ratio
    pub filter_ratio: f64,
    /// Scan ranges (file/tablet count)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub scan_ranges: Option<u64>,
    /// Bytes read
    #[serde(skip_serializing_if = "Option::is_none")]
    pub bytes_read: Option<u64>,
    /// IO wait time
    #[serde(skip_serializing_if = "Option::is_none")]
    pub io_time_ms: Option<f64>,
    /// Cache hit rate (DataCache for external, PageCache for internal)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub cache_hit_rate: Option<f64>,
    /// Predicates applied
    #[serde(skip_serializing_if = "Option::is_none")]
    pub predicates: Option<String>,
    /// Partition pruning info
    #[serde(skip_serializing_if = "Option::is_none")]
    pub partitions_scanned: Option<String>,
    /// For external tables: catalog.database.table format
    #[serde(skip_serializing_if = "Option::is_none")]
    pub full_table_path: Option<String>,
}

/// Join operator details
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JoinDetailForLLM {
    pub plan_node_id: i32,
    /// HASH_JOIN, CROSS_JOIN, etc.
    pub join_type: String,
    /// Build side rows
    pub build_rows: u64,
    /// Probe side rows
    pub probe_rows: u64,
    /// Output rows
    pub output_rows: u64,
    /// Hash table memory
    #[serde(skip_serializing_if = "Option::is_none")]
    pub hash_table_memory: Option<u64>,
    /// Is broadcast join
    #[serde(default)]
    pub is_broadcast: bool,
    /// Runtime filter info
    #[serde(skip_serializing_if = "Option::is_none")]
    pub runtime_filter: Option<String>,
}

/// Aggregation operator details
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AggDetailForLLM {
    pub plan_node_id: i32,
    /// Input rows
    pub input_rows: u64,
    /// Output rows after aggregation
    pub output_rows: u64,
    /// Aggregation ratio (output/input)
    pub agg_ratio: f64,
    /// GROUP BY keys
    #[serde(skip_serializing_if = "Option::is_none")]
    pub group_by_keys: Option<String>,
    /// Hash table memory
    #[serde(skip_serializing_if = "Option::is_none")]
    pub hash_table_memory: Option<u64>,
    /// Is streaming agg
    #[serde(default)]
    pub is_streaming: bool,
}

/// Exchange (shuffle) operator details
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExchangeDetailForLLM {
    pub plan_node_id: i32,
    /// SHUFFLE, BROADCAST, GATHER
    pub exchange_type: String,
    /// Data sent bytes
    pub bytes_sent: u64,
    /// Rows sent
    pub rows_sent: u64,
    /// Network time
    #[serde(skip_serializing_if = "Option::is_none")]
    pub network_time_ms: Option<f64>,
}

/// Simplified execution plan for LLM
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionPlanForLLM {
    /// DAG description in text format
    /// e.g., "SCAN(orders) -> JOIN -> SCAN(customers) -> AGG -> SINK"
    pub dag_description: String,
    /// Hotspot nodes (time_percentage > 15%)
    #[serde(default)]
    pub hotspot_nodes: Vec<HotspotNodeForLLM>,
}

/// Hotspot node information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HotspotNodeForLLM {
    /// Operator name, e.g., "HASH_JOIN"
    pub operator: String,
    /// Plan node ID
    pub plan_node_id: i32,
    /// Time percentage (0-100)
    pub time_percentage: f64,
    /// Key metrics relevant to this operator
    #[serde(default, skip_serializing_if = "HashMap::is_empty")]
    pub key_metrics: HashMap<String, String>,
    /// Upstream operator names
    #[serde(default)]
    pub upstream_operators: Vec<String>,
}

/// Rule engine diagnostic result for LLM
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiagnosticForLLM {
    /// Rule ID, e.g., "S001"
    pub rule_id: String,
    /// Severity: Error/Warning/Info
    pub severity: String,
    /// Affected operator
    pub operator: String,
    /// Plan node ID
    #[serde(skip_serializing_if = "Option::is_none")]
    pub plan_node_id: Option<i32>,
    /// Diagnostic message
    pub message: String,
    /// Evidence that triggered the rule
    #[serde(default, skip_serializing_if = "HashMap::is_empty")]
    pub evidence: HashMap<String, String>,
    /// Threshold metadata for traceability (how the threshold was determined)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub threshold_info: Option<ThresholdInfoForLLM>,
}

/// Threshold information for LLM to understand how diagnostics were triggered
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThresholdInfoForLLM {
    /// Threshold value used (in appropriate unit, e.g., ms for time)
    pub threshold_value: f64,
    /// Source: "baseline" (adaptive from history) or "default" (static config)
    pub source: String,
    /// If baseline was used, P95 value from historical data (ms)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub baseline_p95_ms: Option<f64>,
    /// Number of samples used to calculate baseline
    #[serde(skip_serializing_if = "Option::is_none")]
    pub sample_count: Option<usize>,
}

/// Key performance metrics
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct KeyMetricsForLLM {
    /// Data skew metrics
    #[serde(skip_serializing_if = "Option::is_none")]
    pub skew_metrics: Option<SkewMetricsForLLM>,
    /// IO metrics
    #[serde(skip_serializing_if = "Option::is_none")]
    pub io_metrics: Option<IOMetricsForLLM>,
    /// Memory metrics
    #[serde(skip_serializing_if = "Option::is_none")]
    pub memory_metrics: Option<MemoryMetricsForLLM>,
    /// Cardinality estimation errors
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub cardinality_errors: Vec<CardinalityErrorForLLM>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SkewMetricsForLLM {
    pub max_rows: u64,
    pub min_rows: u64,
    pub avg_rows: f64,
    pub skew_ratio: f64,
    pub affected_operator: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IOMetricsForLLM {
    pub total_bytes_read: u64,
    pub cache_hit_rate: f64,
    pub io_time_percentage: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryMetricsForLLM {
    pub peak_memory_bytes: u64,
    pub spill_bytes: u64,
    pub hash_table_memory: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CardinalityErrorForLLM {
    pub operator: String,
    pub estimated_rows: u64,
    pub actual_rows: u64,
    pub error_ratio: f64,
}

// ============================================================================
// Response Types
// ============================================================================

/// Root Cause Analysis Response from LLM
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RootCauseAnalysisResponse {
    /// Identified root causes
    #[serde(default)]
    pub root_causes: Vec<LLMRootCause>,
    /// Causal chains with explanations
    #[serde(default)]
    pub causal_chains: Vec<LLMCausalChain>,
    /// Prioritized recommendations
    #[serde(default)]
    pub recommendations: Vec<LLMRecommendation>,
    /// Summary in natural language
    #[serde(default)]
    pub summary: String,
    /// Hidden issues not detected by rule engine
    #[serde(default)]
    pub hidden_issues: Vec<LLMHiddenIssue>,
}

impl LLMAnalysisResponseTrait for RootCauseAnalysisResponse {
    fn summary(&self) -> &str {
        &self.summary
    }

    fn confidence(&self) -> Option<f64> {
        if self.root_causes.is_empty() {
            None
        } else {
            Some(
                self.root_causes.iter().map(|r| r.confidence).sum::<f64>()
                    / self.root_causes.len() as f64,
            )
        }
    }
}

/// Root cause identified by LLM
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMRootCause {
    /// Unique ID for this root cause
    pub root_cause_id: String,
    /// Description of the root cause
    pub description: String,
    /// Confidence score (0.0 - 1.0)
    pub confidence: f64,
    /// Evidence supporting this conclusion
    #[serde(default)]
    pub evidence: Vec<String>,
    /// Symptom rule IDs caused by this root cause
    #[serde(default)]
    pub symptoms: Vec<String>,
    /// Whether this is an implicit root cause (not detected by rules)
    #[serde(default)]
    pub is_implicit: bool,
}

/// Causal chain with explanation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMCausalChain {
    /// Chain representation, e.g., ["ç»Ÿè®¡ä¿¡æ¯è¿‡æœŸ", "â†’", "Joiné¡ºåºä¸ä¼˜", "â†’", "å†…å­˜è¿‡é«˜"]
    pub chain: Vec<String>,
    /// Natural language explanation
    pub explanation: String,
}

/// Recommendation from LLM
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMRecommendation {
    /// Priority (1 = highest)
    pub priority: u32,
    /// Action to take
    pub action: String,
    /// Expected improvement
    #[serde(default)]
    pub expected_improvement: String,
    /// SQL example if applicable
    #[serde(skip_serializing_if = "Option::is_none")]
    pub sql_example: Option<String>,
}

/// Hidden issue not detected by rule engine
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMHiddenIssue {
    /// Issue description
    pub issue: String,
    /// Suggested action
    pub suggestion: String,
}

// ============================================================================
// Builder for RootCauseAnalysisRequest
// ============================================================================

impl RootCauseAnalysisRequest {
    /// Create a new builder
    pub fn builder() -> RootCauseAnalysisRequestBuilder {
        RootCauseAnalysisRequestBuilder::default()
    }
}

#[derive(Default)]
pub struct RootCauseAnalysisRequestBuilder {
    query_summary: Option<QuerySummaryForLLM>,
    profile_data: Option<ProfileDataForLLM>,
    execution_plan: Option<ExecutionPlanForLLM>,
    rule_diagnostics: Vec<DiagnosticForLLM>,
    key_metrics: KeyMetricsForLLM,
    user_question: Option<String>,
}

impl RootCauseAnalysisRequestBuilder {
    pub fn query_summary(mut self, summary: QuerySummaryForLLM) -> Self {
        self.query_summary = Some(summary);
        self
    }

    pub fn profile_data(mut self, data: ProfileDataForLLM) -> Self {
        self.profile_data = Some(data);
        self
    }

    pub fn execution_plan(mut self, plan: ExecutionPlanForLLM) -> Self {
        self.execution_plan = Some(plan);
        self
    }

    pub fn add_diagnostic(mut self, diag: DiagnosticForLLM) -> Self {
        self.rule_diagnostics.push(diag);
        self
    }

    pub fn diagnostics(mut self, diags: Vec<DiagnosticForLLM>) -> Self {
        self.rule_diagnostics = diags;
        self
    }

    pub fn key_metrics(mut self, metrics: KeyMetricsForLLM) -> Self {
        self.key_metrics = metrics;
        self
    }

    pub fn user_question(mut self, question: impl Into<String>) -> Self {
        self.user_question = Some(question.into());
        self
    }

    pub fn build(self) -> Result<RootCauseAnalysisRequest, &'static str> {
        Ok(RootCauseAnalysisRequest {
            query_summary: self.query_summary.ok_or("query_summary is required")?,
            profile_data: self.profile_data,
            execution_plan: self.execution_plan.ok_or("execution_plan is required")?,
            rule_diagnostics: self.rule_diagnostics,
            key_metrics: self.key_metrics,
            user_question: self.user_question,
        })
    }
}

// ============================================================================
// Helper Functions
// ============================================================================

/// Determine table type based on CATALOG prefix, not scan operator type!
///
/// StarRocks has two deployment modes:
/// 1. Shared-Nothing (å­˜ç®—ä¸€ä½“): internal tables use OLAP_SCAN
/// 2. Shared-Data (å­˜ç®—åˆ†ç¦»): internal tables use CONNECTOR_SCAN
///
/// Both modes can access external tables (Hive/Iceberg/ES etc.) via catalogs.
///
/// ## The ONLY reliable rule:
/// - `default_catalog` â†’ internal (StarRocks native table)
/// - Any other catalog name â†’ external (foreign table)
///
/// # Arguments
/// * `table_name` - Full table name, may be "catalog.database.table" or "database.table" or just "table"
///
/// # Returns
/// * "internal" - StarRocks native table (in default_catalog)
/// * "external" - External table (any non-default catalog)
pub fn determine_table_type(table_name: &str) -> String {
    match table_name.split('.').collect::<Vec<_>>() {
        parts if parts.len() >= 3 => {
            if parts[0].eq_ignore_ascii_case("default_catalog") {
                "internal".to_string()
            } else {
                "external".to_string()
            }
        },
        parts if parts.len() == 2 => "internal".to_string(),
        _ => "internal".to_string(),
    }
}

/// Determine external table connector type from Profile metrics
///
/// StarRocks Profile ä¸­å„ç±»å¤–è¡¨çš„æ ‡è¯† (from be/src/exec/hdfs_scanner):
/// - **Iceberg**: Has "IcebergV2FormatTimer" section under ORC/Parquet
/// - **Hive**: Has "ORC" or "Parquet" section, but NO Iceberg indicators
/// - **Delta Lake**: Has "DeletionVector" section (Delta uses deletion vectors)
/// - **Hudi**: Has Hudi-specific metrics
/// - **Paimon**: Has Paimon-specific metrics (uses deletion vector too)
/// - **JDBC**: Has JDBC-related metrics
/// - **ES/Elasticsearch**: Has ES-specific metrics
///
/// # Arguments
/// * `metrics` - The unique_metrics map from SCAN node
///
/// # Returns
/// * "iceberg", "hive", "hudi", "paimon", "deltalake", "jdbc", "es", or "unknown"
pub fn determine_connector_type(metrics: &std::collections::HashMap<String, String>) -> String {
    let keys_str = metrics
        .keys()
        .map(|k| k.to_lowercase())
        .collect::<Vec<_>>()
        .join(" ");
    let has = |p: &str| keys_str.contains(p);
    match () {
        _ if has("iceberg") || has("deletefilebuild") => "iceberg",
        _ if has("deletionvector") => "deltalake",
        _ if has("hudi") => "hudi",
        _ if has("paimon") => "paimon",
        _ if has("jdbc") => "jdbc",
        _ if has("elasticsearch") || has("_es_") => "es",
        _ if ["orc", "parquet", "stripe", "rowgroup"]
            .iter()
            .any(|p| has(p)) =>
        {
            "hive"
        },
        _ => "unknown",
    }
    .to_string()
}
